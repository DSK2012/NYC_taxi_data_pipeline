{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9371e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "            master('local[*]').\\\n",
    "            config(\"spark.driver.memory\", \"5g\").\\\n",
    "            appName('project').\\\n",
    "            config(\"spark.jars\", \"postgresql-42.7.0.jar\").\\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b2f05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aeab321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5481a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.ParquetFile(\"data/yellowData/yellow_tripdata_2023-05.parquet\").schema\n",
    "# int64 - long\n",
    "# int32 - int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04f0c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.ParquetFile(\"data/greenData/green_tripdata_2023-05.parquet\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1761ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df_schema = StructType([\n",
    "    StructField('VendorId', LongType()),\n",
    "    StructField('tpep_pickup_datetime', TimestampType()),\n",
    "    StructField('tpep_dropoff_datetime', TimestampType()),\n",
    "    StructField('passenger_count', LongType()),\n",
    "    StructField('trip_distance',DoubleType()),\n",
    "    StructField('RatecodeID',LongType()),\n",
    "    StructField('store_and_fwd_flag',StringType()),\n",
    "    StructField('PULocationID', IntegerType()),\n",
    "    StructField('DOLocationID', IntegerType()),\n",
    "    StructField('payment_type', LongType()),\n",
    "    StructField('fare_amount',DoubleType()),\n",
    "    StructField('extra',DoubleType()),\n",
    "    StructField('mta_tax',DoubleType()),\n",
    "    StructField('tip_amount',DoubleType()),\n",
    "    StructField('tolls_amount',DoubleType()),\n",
    "    StructField('improvement_surcharge',DoubleType()),\n",
    "    StructField('total_amount',DoubleType()),\n",
    "    StructField('congestion_surcharge',DoubleType()),\n",
    "    StructField('airport_fee',DoubleType()),\n",
    "])\n",
    "\n",
    "green_df_schema = StructType([\n",
    "    StructField('VendorId', IntegerType()),\n",
    "    StructField('lpep_pickup_datetime', TimestampType()),\n",
    "    StructField('lpep_dropoff_datetime', TimestampType()),\n",
    "    StructField('store_and_fwd_flag',StringType()),\n",
    "    StructField('RatecodeID',LongType()),\n",
    "    StructField('PULocationID', IntegerType()),\n",
    "    StructField('DOLocationID', IntegerType()),\n",
    "    StructField('passenger_count', LongType()),\n",
    "    StructField('trip_distance',DoubleType()),\n",
    "    StructField('fare_amount',DoubleType()),\n",
    "    StructField('extra',DoubleType()),\n",
    "    StructField('mta_tax',DoubleType()),\n",
    "    StructField('tip_amount',DoubleType()),\n",
    "    StructField('tolls_amount',DoubleType()),\n",
    "    StructField('ehail_fee', DoubleType()),\n",
    "    StructField('improvement_surcharge',DoubleType()),\n",
    "    StructField('total_amount',DoubleType()),\n",
    "    StructField('payment_type', LongType()),\n",
    "    StructField('trip_type', LongType()),\n",
    "    StructField('congestion_surcharge',DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f0d6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow_df = spark.read.schema(yellow_df_schema).parquet(\"data/yellowData/*.parquet\")\n",
    "# green_df = spark.read.schema(green_df_schema).parquet(\"data/greenData/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46436fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f312f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filenames_func(directory_path):\n",
    "    \n",
    "    files = [i for i in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, i))]\n",
    "    files_filtered = [os.path.join(directory_path,f) for f in files if not f.startswith('.')]\n",
    "    \n",
    "    return files_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2631b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file_paths):\n",
    "    new_df = spark.read.parquet(file_paths[0])\n",
    "    for i in file_paths[1:5]:\n",
    "        new_df = new_df.\\\n",
    "                    union(spark.read.\\\n",
    "                    parquet(i))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f08e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df = create_df(filenames_func('./data/yellowData/'))\n",
    "# green_df = create_df(filenames_func('./data/greenData/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eccea968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in yellow taxi data :  15840450\n"
     ]
    }
   ],
   "source": [
    "print('number of records in yellow taxi data : ', yellow_df.count())\n",
    "# print('number of records in green taxi data : ', green_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "549d1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 261:=====================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|VendorID|\n",
      "+--------+\n",
      "|       1|\n",
      "|       6|\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('VendorID').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa4e538c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VendorID', 'int'),\n",
       " ('tpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('tpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('passenger_count', 'bigint'),\n",
       " ('trip_distance', 'double'),\n",
       " ('RatecodeID', 'bigint'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'double'),\n",
       " ('Airport_fee', 'double')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3514c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = ['VendorID',\n",
    "             'passenger_count',\n",
    "             'RateCodeID',\n",
    "             'store_and_fwd_flag',\n",
    "             'payment_type',\n",
    "             'Airport_fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "97aa8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID 3 [1, 6, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count 11 [0, 7, 6, 9, 5, 1, 3, 8, 2, 4, None]\n",
      "RateCodeID 8 [6, 5, 1, 3, 2, 4, 99, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 273:====================================================>  (38 + 2) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_and_fwd_flag 3 ['Y', 'N', None]\n",
      "payment_type 6 [0, 5, 1, 3, 2, 4]\n",
      "Airport_fee 8 [0.0, -1.75, 1.7, None, 1.75, 1.25, 1.0, -1.25]\n"
     ]
    }
   ],
   "source": [
    "for i in temp_list:\n",
    "    distinct_values_list = yellow_df.select(i).distinct().collect()\n",
    "    print(i, len(distinct_values_list), [i[0] for i in distinct_values_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c9f3aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all column names\n",
    "for col in yellow_df.columns:\n",
    "    yellow_df = yellow_df.withColumnRenamed(col, col.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "632caca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vendorid', 'int'),\n",
       " ('tpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('tpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('passenger_count', 'bigint'),\n",
       " ('trip_distance', 'double'),\n",
       " ('ratecodeid', 'bigint'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('pulocationid', 'int'),\n",
       " ('dolocationid', 'int'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'double'),\n",
       " ('airport_fee', 'double')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "98e85b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10181689"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('datetimedim.parquet').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d824dc",
   "metadata": {},
   "source": [
    "## Vendor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e50d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# VendorID\n",
    "# cast to int\n",
    "yellow_df = yellow_df.withColumn('vendorid', col('vendorid').cast(IntegerType()))\n",
    "# can only have value 1 or 2 acc to data dictionary, drop other rows\n",
    "yellow_df = yellow_df.filter(col('vendorid').isin(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9ac0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15837138"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bf0366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=====================================================>  (38 + 2) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|vendorid|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('vendorid').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230aaf5e",
   "metadata": {},
   "source": [
    "## Passenger Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82f4fa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|passenger_count|   count|\n",
      "+---------------+--------+\n",
      "|              0|  256536|\n",
      "|              7|      31|\n",
      "|           NULL|  462033|\n",
      "|              6|  132533|\n",
      "|              9|      27|\n",
      "|              5|  194646|\n",
      "|              1|11473702|\n",
      "|              3|  612233|\n",
      "|              8|     114|\n",
      "|              2| 2350579|\n",
      "|              4|  354704|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 70:======================================================> (39 + 1) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('passenger_count').groupBy('passenger_count').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f993bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passenger Count\n",
    "# cast to int\n",
    "yellow_df = yellow_df.withColumn('passenger_count', col('passenger_count').\\\n",
    "                                 cast(IntegerType()))\n",
    "#remove None records\n",
    "yellow_df = yellow_df.dropna(how = 'all', subset = ['passenger_count'])\n",
    "#remove records with 0 as passenger count\n",
    "yellow_df = yellow_df.filter(col('passenger_count') > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba85281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=====================================================>  (38 + 2) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|passenger_count|   count|\n",
      "+---------------+--------+\n",
      "|              1|11473702|\n",
      "|              6|  132533|\n",
      "|              3|  612233|\n",
      "|              5|  194646|\n",
      "|              9|      27|\n",
      "|              4|  354704|\n",
      "|              8|     114|\n",
      "|              7|      31|\n",
      "|              2| 2350579|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('passenger_count').groupBy('passenger_count').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c684e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15118569"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a76cbd",
   "metadata": {},
   "source": [
    "## Trip Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "122e60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "193152"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.filter(col('trip_distance') == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0d3d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to float\n",
    "yellow_df = yellow_df.withColumn('trip_distance', col('trip_distance').cast(FloatType()))\n",
    "#drop rows with trip distance = 0\n",
    "yellow_df = yellow_df.filter((col('trip_distance') != 0) & (col('trip_distance') > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7edfb12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14925417"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a21b0",
   "metadata": {},
   "source": [
    "## Location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39cda609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ccebb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function isnull in module pyspark.sql.functions:\n",
      "\n",
      "isnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "    An expression that returns true if the column is null.\n",
      "    \n",
      "    .. versionadded:: 1.6.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    col : :class:`~pyspark.sql.Column` or str\n",
      "        target column to compute on.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        True if value is null and False otherwise.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "    >>> df.select(\"a\", \"b\", isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\")).show()\n",
      "    +----+----+-----+-----+\n",
      "    |   a|   b|   r1|   r2|\n",
      "    +----+----+-----+-----+\n",
      "    |   1|NULL|false| true|\n",
      "    |NULL|   2| true|false|\n",
      "    +----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(isnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17cdb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.\\\n",
    "    select('pulocationid').\\\n",
    "    filter( (col('pulocationid') <1) | (isnull(col('pulocationid'))) ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc53a26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.\\\n",
    "    select('dolocationid').\\\n",
    "    filter( (col('dolocationid') <1) | (isnull(col('dolocationid'))) ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e4a48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast location id to int\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('pulocationid', col('pulocationid').cast(IntegerType())).\\\n",
    "                withColumn('dolocationid', col('dolocationid').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "609fd0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vendorid', 'int'),\n",
       " ('tpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('tpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('passenger_count', 'int'),\n",
       " ('trip_distance', 'float'),\n",
       " ('ratecodeid', 'bigint'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('pulocationid', 'int'),\n",
       " ('dolocationid', 'int'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'double'),\n",
       " ('airport_fee', 'double')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af62e8e4",
   "metadata": {},
   "source": [
    "## Rate Code ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e81c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cast ratecode id to int\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('ratecodeid', col('ratecodeid').cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f37b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop rows with invalid ratecode id\n",
    "yellow_df = yellow_df.\\\n",
    "                filter(col('ratecodeid').isin(1,2,3,4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275247e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1efb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24c076f",
   "metadata": {},
   "source": [
    "## Store & fwd flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff617e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6fcf803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:==================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|store_and_fwd_flag|\n",
      "+------------------+\n",
      "|                 Y|\n",
      "|                 N|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('store_and_fwd_flag').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ec1ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:===================================================>    (37 + 3) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|store_and_fwd_flag|   count|\n",
      "+------------------+--------+\n",
      "|                 Y|   85369|\n",
      "|                 N|14765088|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('store_and_fwd_flag').groupBy('store_and_fwd_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26a57ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert values to lowercase\n",
    "yellow_df = yellow_df.withColumn('store_and_fwd_flag', lower(col('store_and_fwd_flag')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cba0ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|store_and_fwd_flag|   count|\n",
      "+------------------+--------+\n",
      "|                 n|14765088|\n",
      "|                 y|   85369|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('store_and_fwd_flag').groupBy('store_and_fwd_flag').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa11896",
   "metadata": {},
   "source": [
    "## Payment Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38d4b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:=================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|payment_type|   count|\n",
      "+------------+--------+\n",
      "|           1|11919747|\n",
      "|           3|   72289|\n",
      "|           2| 2672504|\n",
      "|           4|  185917|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.\\\n",
    "    select('payment_type').\\\n",
    "    groupBy('payment_type').\\\n",
    "    count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac4445",
   "metadata": {},
   "source": [
    "## Fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63ebad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136941"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.\\\n",
    "    select('fare_amount').\\\n",
    "    filter(col('fare_amount') < 0).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6caf42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "## adding a column saying whether the fare amount is +ve or -ve\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('errordata',when(col('fare_amount') < 0, 'y').\\\n",
    "                otherwise('n'))\n",
    "\n",
    "## casting the column to double\n",
    "\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('fare_amount', col('fare_amount').cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c395f6",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af3c9385",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:==================================================>    (37 + 3) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|extra|  count|\n",
      "+-----+-------+\n",
      "|  0.0|5611620|\n",
      "|  3.5| 852177|\n",
      "|10.25|  18756|\n",
      "| -5.0|   5112|\n",
      "| 12.5|      4|\n",
      "|  4.5|     25|\n",
      "| 0.05|      2|\n",
      "| -1.0|  42563|\n",
      "| 9.25|  66190|\n",
      "|  0.7|     17|\n",
      "|  2.5|3692502|\n",
      "| 2.15|      3|\n",
      "|  1.0|2988270|\n",
      "| 2.45|     64|\n",
      "|  2.7|      1|\n",
      "|  2.8|      4|\n",
      "| -6.0|   1934|\n",
      "| 0.04|      5|\n",
      "| 7.45|      4|\n",
      "|  7.5| 134700|\n",
      "| 0.75|     52|\n",
      "|  3.8|      1|\n",
      "| 11.0|      6|\n",
      "| 2.75|  26202|\n",
      "|  8.5|   3068|\n",
      "| 2.25|    804|\n",
      "|  3.2|    254|\n",
      "|14.25|     22|\n",
      "| -2.5|  22110|\n",
      "|12.75|     22|\n",
      "| 0.03|      3|\n",
      "| 6.75|  25228|\n",
      "|  2.0|      2|\n",
      "|  1.5|     65|\n",
      "| 0.25|     12|\n",
      "|  8.2|      1|\n",
      "| 4.25|  71144|\n",
      "| 3.25|     85|\n",
      "| 0.01|      3|\n",
      "| 10.0|   4459|\n",
      "| 5.75|      6|\n",
      "| 0.11|      2|\n",
      "| 0.02|      9|\n",
      "|  6.0| 138174|\n",
      "| 1.75|  35158|\n",
      "| 7.75|  16262|\n",
      "| -7.5|   1191|\n",
      "|  5.0|1070836|\n",
      "|11.75|  13798|\n",
      "|-4.25|      1|\n",
      "|-0.25|      1|\n",
      "| -3.5|      4|\n",
      "| 5.25|    314|\n",
      "| 0.06|      1|\n",
      "|  7.0|      7|\n",
      "| 1.25|   1112|\n",
      "|  1.7|      1|\n",
      "|-0.75|      6|\n",
      "| 4.75|     13|\n",
      "|  3.1|      1|\n",
      "|  0.5|      5|\n",
      "| 2.95|      1|\n",
      "|  3.0|     25|\n",
      "| 1.12|      1|\n",
      "|67.33|      1|\n",
      "|-10.0|      1|\n",
      "| 0.07|      2|\n",
      "|  6.5|      1|\n",
      "| 0.12|      1|\n",
      "| 1.01|      2|\n",
      "|  4.0|      1|\n",
      "| 0.08|      1|\n",
      "| 0.15|      1|\n",
      "|  6.3|      1|\n",
      "|16.25|      1|\n",
      "|  5.5|      1|\n",
      "| 8.75|   1622|\n",
      "| 9.75|    522|\n",
      "| 7.25|    471|\n",
      "|-3.25|      1|\n",
      "| 6.25|    691|\n",
      "| 2.05|      2|\n",
      "|13.75|      1|\n",
      "|96.38|      1|\n",
      "|11.25|    224|\n",
      "| 4.95|      1|\n",
      "| 3.75|   2471|\n",
      "| 1.02|      1|\n",
      "| 1.05|      2|\n",
      "|  0.1|      1|\n",
      "| 0.09|      1|\n",
      "|  5.3|      1|\n",
      "|  4.7|      1|\n",
      "| 0.13|      1|\n",
      "|  5.6|      1|\n",
      "| 1.06|      2|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "yellow_df.select('extra').groupBy('extra').count().show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a50c8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 109:=============================================>         (33 + 7) / 40]\r",
      "\r",
      "[Stage 109:=================================================>     (36 + 4) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|mta_tax|   count|\n",
      "+-------+--------+\n",
      "|    0.0|  101294|\n",
      "|   0.05|     232|\n",
      "|    4.0|      14|\n",
      "|    0.5|14613701|\n",
      "|  -0.05|       6|\n",
      "|   -0.5|  135204|\n",
      "|   5.75|       1|\n",
      "|   1.05|       1|\n",
      "|    3.5|       1|\n",
      "|    0.8|       1|\n",
      "|   1.53|       1|\n",
      "|   3.25|       1|\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 109:=====================================================> (39 + 1) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.select('mta_tax').groupBy('mta_tax').count().show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcfd9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow_df.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c97cb416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'ratecodeid',\n",
       " 'store_and_fwd_flag',\n",
       " 'pulocationid',\n",
       " 'dolocationid',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'errordata']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19a652bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14712990"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.\\\n",
    "    filter(\n",
    "        (col('extra') >= 0) &\n",
    "        (col('mta_tax') >= 0) &\n",
    "        (col('tip_amount') >= 0) &\n",
    "        (col('tolls_amount') >= 0) &\n",
    "        (col('improvement_surcharge') >= 0) &\n",
    "        (col('total_amount') >= 0) &\n",
    "        (col('congestion_surcharge') >= 0) &\n",
    "        (col('airport_fee') >= 0) \n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46910fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14850457"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "082de4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('X',)], schema = 'name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73f742ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = col('name') == 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c78044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|name|test|\n",
      "+----+----+\n",
      "|   X|   1|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('test', when(cond, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40d1c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None. The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "    +-----+---+\n",
      "    |   _1| _2|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries.\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  1|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Create a DataFrame with column names specified.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create an empty DataFrame.\n",
      "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "    as the DataFrame lacks data from which the schema can be inferred.\n",
      "    \n",
      "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    +----+---+\n",
      "    \n",
      "    Create a DataFrame from Row objects.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "    >>> df.show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    +---+---+\n",
      "    |  0|  1|\n",
      "    +---+---+\n",
      "    |  1|  2|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7349f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_negative_condition =(\n",
    "                            (col('fare_amount') >= 0)& \n",
    "                            (col('extra') >= 0) &\n",
    "                            (col('mta_tax') >= 0) &\n",
    "                            (col('tip_amount') >= 0) &\n",
    "                            (col('tolls_amount') >= 0) &\n",
    "                            (col('improvement_surcharge') >= 0) &\n",
    "                            (col('total_amount') >= 0) &\n",
    "                            (col('congestion_surcharge') >= 0) &\n",
    "                            (col('airport_fee') >= 0) \n",
    "    )\n",
    "\n",
    "yellow_df = yellow_df.\\\n",
    "        withColumn('errordata',when(non_negative_condition, 'n').\\\n",
    "        otherwise('y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7dcf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14850457"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d16047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 124:==================================================>    (37 + 3) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|errordata|   count|\n",
      "+---------+--------+\n",
      "|        n|14712990|\n",
      "|        y|  137467|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.groupby('errordata').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18978afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "# yellow_df.\\\n",
    "#     select('vendorid').\\\n",
    "#     distinct().\\\n",
    "#     withColumn('vendor_key', monotonically_increasing_id()).\\\n",
    "#     withColumn('vendor_name', )\n",
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "399de359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function monotonically_increasing_id in module pyspark.sql.functions:\n",
      "\n",
      "monotonically_increasing_id() -> pyspark.sql.column.Column\n",
      "    A column that generates monotonically increasing 64-bit integers.\n",
      "    \n",
      "    The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "    The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "    within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "    less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "    \n",
      "    .. versionadded:: 1.6.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The function is non-deterministic because its result depends on partition IDs.\n",
      "    \n",
      "    As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "    This expression would return the following IDs:\n",
      "    0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`~pyspark.sql.Column`\n",
      "        last value of the group.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import functions as sf\n",
      "    >>> spark.range(0, 10, 1, 2).select(sf.monotonically_increasing_id()).show()\n",
      "    +-----------------------------+\n",
      "    |monotonically_increasing_id()|\n",
      "    +-----------------------------+\n",
      "    |                            0|\n",
      "    |                            1|\n",
      "    |                            2|\n",
      "    |                            3|\n",
      "    |                            4|\n",
      "    |                   8589934592|\n",
      "    |                   8589934593|\n",
      "    |                   8589934594|\n",
      "    |                   8589934595|\n",
      "    |                   8589934596|\n",
      "    +-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "help(monotonically_increasing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c40637a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def vendor_name_udf(vendorid):\n",
    "    vendor_dict = {\n",
    "                    1 : 'Creative Mobile Technologies, LLC',\n",
    "                   2 : 'VeriFone Inc.'\n",
    "    }\n",
    "    if vendorid is not None:\n",
    "        return vendor_dict[vendorid]\n",
    "\n",
    "vendor_DIM = yellow_df.\\\n",
    "    select('vendorid').\\\n",
    "    distinct().\\\n",
    "    withColumn('vendor_key', monotonically_increasing_id()).\\\n",
    "    withColumn('vendor_name', vendor_name_udf(col('vendorid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a654db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|vendorid|vendor_key|         vendor_name|\n",
      "+--------+----------+--------------------+\n",
      "|       1|         0|Creative Mobile T...|\n",
      "|       2|         1|       VeriFone Inc.|\n",
      "+--------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vendor_DIM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe1bc0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'ratecodeid',\n",
       " 'store_and_fwd_flag',\n",
       " 'pulocationid',\n",
       " 'dolocationid',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'errordata']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "243b7cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp_ntz]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu_times = yellow_df.\\\n",
    "    select(col('tpep_pickup_datetime').alias('timestamp')).\\\n",
    "    dropDuplicates()\n",
    "pu_times.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79cac244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp_ntz]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_times = yellow_df.\\\n",
    "    select(col('tpep_dropoff_datetime').alias('timestamp')).\\\n",
    "    dropDuplicates()\n",
    "do_times.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0097dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_DIM = pu_times.\\\n",
    "    union(do_times).\\\n",
    "    dropDuplicates().\\\n",
    "    withColumn('datetime_key', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0ada275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7901697"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_times.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0249380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7904168"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu_times.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3b1594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|          timestamp|datetime_key|\n",
      "+-------------------+------------+\n",
      "|2023-06-01 00:48:24|           0|\n",
      "|2023-06-01 00:38:30|           1|\n",
      "|2023-06-01 00:02:52|           2|\n",
      "|2023-06-01 00:35:38|           3|\n",
      "|2023-06-01 00:33:21|           4|\n",
      "|2023-06-01 00:23:20|           5|\n",
      "|2023-06-01 00:25:58|           6|\n",
      "|2023-06-01 00:09:42|           7|\n",
      "|2023-06-01 01:07:01|           8|\n",
      "|2023-06-01 01:30:58|           9|\n",
      "|2023-06-01 01:26:11|          10|\n",
      "|2023-06-01 01:09:09|          11|\n",
      "|2023-06-01 01:19:48|          12|\n",
      "|2023-06-01 02:16:29|          13|\n",
      "|2023-06-01 03:22:23|          14|\n",
      "|2023-06-01 04:18:30|          15|\n",
      "|2023-06-01 04:12:21|          16|\n",
      "|2023-06-01 05:16:51|          17|\n",
      "|2023-06-01 06:39:13|          18|\n",
      "|2023-06-01 06:00:09|          19|\n",
      "+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datetime_DIM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99d4cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10181689"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_DIM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "760753a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'ratecodeid',\n",
       " 'store_and_fwd_flag',\n",
       " 'pulocationid',\n",
       " 'dolocationid',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'errordata']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b039fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_locations = yellow_df.\\\n",
    "    select(col('pulocationid').alias('locationid')).\\\n",
    "    dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5102bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_locations = yellow_df.\\\n",
    "    select(col('dolocationid').alias('locationid')).\\\n",
    "    dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8d9a8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxizone_DIM = pu_locations.\\\n",
    "    union(do_locations).\\\n",
    "    dropDuplicates().\\\n",
    "    withColumn('zone_key', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8677fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxizone_DIM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3800b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def paymenttype_name_udf(payment_type):\n",
    "    payment_type_dict = {\n",
    "                    1 : 'Credit card',\n",
    "                    2 : 'Cash',\n",
    "                    3 : 'No charge',\n",
    "                    4 : 'Dispute',\n",
    "                    5 : 'Unknown',\n",
    "                    6 : 'Voided trip'\n",
    "    }\n",
    "    if payment_type is not None:\n",
    "        return payment_type_dict[payment_type]\n",
    "\n",
    "paymenttype_DIM = yellow_df.\\\n",
    "    select('payment_type').\\\n",
    "    distinct().\\\n",
    "    withColumn('paymenttype_key', monotonically_increasing_id()).\\\n",
    "    withColumn('description', paymenttype_name_udf(col('payment_type')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d4ba2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:==================================================>    (37 + 3) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-----------+\n",
      "|payment_type|paymenttype_key|description|\n",
      "+------------+---------------+-----------+\n",
      "|           1|              0|Credit card|\n",
      "|           3|              1|  No charge|\n",
      "|           2|              2|       Cash|\n",
      "|           4|              3|    Dispute|\n",
      "+------------+---------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paymenttype_DIM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0c576df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def ratecode_name_udf(ratecode):\n",
    "    ratecode_dict = {\n",
    "                    1 : 'Standard rate',\n",
    "                    2 : 'JFK',\n",
    "                    3 : 'Newark',\n",
    "                    4 : 'Nassau or Westchester',\n",
    "                    5 : 'Negotiated fare',\n",
    "                    6 : 'Group ride'\n",
    "    }\n",
    "    if ratecode is not None:\n",
    "        return ratecode_dict[ratecode]\n",
    "\n",
    "ratecode_DIM = yellow_df.\\\n",
    "    select('ratecodeid').\\\n",
    "    distinct().\\\n",
    "    withColumn('ratecode_key', monotonically_increasing_id()).\\\n",
    "    withColumn('description', ratecode_name_udf(col('ratecodeid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a0faad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:====================================================>  (38 + 2) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|ratecodeid|ratecode_key|         description|\n",
      "+----------+------------+--------------------+\n",
      "|         1|           0|       Standard rate|\n",
      "|         6|           1|          Group ride|\n",
      "|         3|           2|              Newark|\n",
      "|         5|           3|     Negotiated fare|\n",
      "|         4|           4|Nassau or Westche...|\n",
      "|         2|           5|                 JFK|\n",
      "+----------+------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratecode_DIM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b953f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee', 'errordata']\n",
      "['timestamp', 'datetime_key']\n"
     ]
    }
   ],
   "source": [
    "print(yellow_df.columns)\n",
    "print(datetime_DIM.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ebd0b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## swapped pickup datetime with putime_key\n",
    "fact_table = yellow_df.join(datetime_DIM, yellow_df.tpep_pickup_datetime == datetime_DIM.timestamp, 'inner').\\\n",
    "                drop('tpep_pickup_datetime','timestamp').\\\n",
    "                withColumnRenamed('datetime_key', 'putime_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a354ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## swapped dropoff datetime with dotime_key\n",
    "fact_table = fact_table.join(datetime_DIM, fact_table.tpep_dropoff_datetime == datetime_DIM.timestamp, 'inner').\\\n",
    "                drop('tpep_dropoff_datetime','timestamp').\\\n",
    "                withColumnRenamed('datetime_key', 'dotime_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7ac7034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## swapped vendor id with vendor_key\n",
    "fact_table = fact_table.join(vendor_DIM, 'vendorid', 'inner').\\\n",
    "                drop('vendorid','vendor_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e320fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## swapped ratecodeid with ratecode_key\n",
    "fact_table = fact_table.join(ratecode_DIM, 'ratecodeid', 'inner').\\\n",
    "                drop('ratecodeid','description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e51bfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pickup location id\n",
    "fact_table = fact_table.join(taxizone_DIM, fact_table.pulocationid == taxizone_DIM.locationid, 'inner').\\\n",
    "                drop('pulocationid','locationid').\\\n",
    "                withColumnRenamed('zone_key', 'puzone_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "864a46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropoff location id\n",
    "fact_table = fact_table.join(taxizone_DIM, fact_table.dolocationid == taxizone_DIM.locationid, 'inner').\\\n",
    "                drop('dolocationid','locationid').\\\n",
    "                withColumnRenamed('zone_key', 'dozone_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e0c4afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## payment type\n",
    "fact_table = fact_table.join(paymenttype_DIM, 'payment_type', 'inner').\\\n",
    "                drop('payment_type','description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ba6aa057",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_in_order = ['vendor_key',\n",
    "                 'putime_key',\n",
    "                 'dotime_key',\n",
    "                 'passenger_count',\n",
    "                 'trip_distance',\n",
    "                 'ratecode_key',\n",
    "                 'store_and_fwd_flag',\n",
    "                 'puzone_key',\n",
    "                 'dozone_key',\n",
    "                 'paymenttype_key',\n",
    "                 'fare_amount',\n",
    "                 'extra',\n",
    "                 'mta_tax',\n",
    "                 'tip_amount',\n",
    "                 'tolls_amount',\n",
    "                 'improvement_surcharge',\n",
    "                 'total_amount',\n",
    "                 'congestion_surcharge',\n",
    "                 'airport_fee',\n",
    "                 'errordata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "051790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ordering columns in the fact table\n",
    "fact_table = fact_table[*cols_in_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0e1026f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/05 21:47:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_table.limit(100).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad982f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2a3be6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10181689"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_DIM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b931770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c636e972",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "DIM_tables_dict = {}\n",
    "yellow_df = yellow_df.withColumn('vendorid', col('vendorid').cast(IntegerType()))\n",
    "# can only have value 1 or 2 acc to data dictionary, drop other rows\n",
    "yellow_df = yellow_df.filter(col('vendorid').isin(1,2))\n",
    "\n",
    "#Passenger Count\n",
    "# cast to int\n",
    "yellow_df = yellow_df.withColumn('passenger_count', col('passenger_count').\\\n",
    "                                cast(IntegerType()))\n",
    "#remove None records\n",
    "yellow_df = yellow_df.dropna(how = 'all', subset = ['passenger_count'])\n",
    "#remove records with 0 as passenger count\n",
    "yellow_df = yellow_df.filter(col('passenger_count') > 0)\n",
    "\n",
    "# cast to float\n",
    "yellow_df = yellow_df.withColumn('trip_distance', col('trip_distance').cast(FloatType()))\n",
    "#drop rows with trip distance = 0\n",
    "yellow_df = yellow_df.filter((col('trip_distance') != 0) & (col('trip_distance') > 0))\n",
    "\n",
    "# cast location id to int\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('pulocationid', col('pulocationid').cast(IntegerType())).\\\n",
    "                withColumn('dolocationid', col('dolocationid').cast(IntegerType()))\n",
    "\n",
    "## cast ratecode id to int\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('ratecodeid', col('ratecodeid').cast(IntegerType()))\n",
    "\n",
    "## drop rows with invalid ratecode id\n",
    "yellow_df = yellow_df.\\\n",
    "                filter(col('ratecodeid').isin(1,2,3,4,5,6))\n",
    "\n",
    "# convert values to lowercase\n",
    "yellow_df = yellow_df.withColumn('store_and_fwd_flag', lower(col('store_and_fwd_flag')))\n",
    "\n",
    "## adding a column saying whether the fare amount is +ve or -ve\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('errordata',when(col('fare_amount') < 0, 'y').\\\n",
    "                otherwise('n'))\n",
    "\n",
    "## casting the column to double\n",
    "\n",
    "yellow_df = yellow_df.\\\n",
    "                withColumn('fare_amount', col('fare_amount').cast(DoubleType()))\n",
    "print(\"at non negative columns\")\n",
    "non_negative_condition =(\n",
    "                        (col('fare_amount') >= 0)& \n",
    "                        (col('extra') >= 0) &\n",
    "                        (col('mta_tax') >= 0) &\n",
    "                        (col('tip_amount') >= 0) &\n",
    "                        (col('tolls_amount') >= 0) &\n",
    "                        (col('improvement_surcharge') >= 0) &\n",
    "                        (col('total_amount') >= 0) &\n",
    "                        (col('congestion_surcharge') >= 0) &\n",
    "                        (col('airport_fee') >= 0) \n",
    ")\n",
    "\n",
    "yellow_df = yellow_df.\\\n",
    "        withColumn('errordata',when(non_negative_condition, 'n').\\\n",
    "        otherwise('y'))\n",
    "\n",
    "## create vendor DIM\n",
    "@udf\n",
    "def vendor_name_udf(vendorid):\n",
    "    vendor_dict = {\n",
    "                    1 : 'Creative Mobile Technologies, LLC',\n",
    "                2 : 'VeriFone Inc.'\n",
    "    }\n",
    "    if vendorid is not None:\n",
    "        return vendor_dict[vendorid]\n",
    "\n",
    "vendor_DIM = yellow_df.\\\n",
    "    select('vendorid').\\\n",
    "    distinct().\\\n",
    "    withColumn('vendor_key', monotonically_increasing_id()).\\\n",
    "    withColumn('vendor_name', vendor_name_udf(col('vendorid')))\n",
    "DIM_tables_dict['vendor_DIM'] = vendor_DIM\n",
    "\n",
    "## create the times DIM\n",
    "pu_times = yellow_df.\\\n",
    "    select(col('tpep_pickup_datetime').alias('timestamp')).\\\n",
    "    dropDuplicates()\n",
    "pu_times.cache()\n",
    "\n",
    "do_times = yellow_df.\\\n",
    "    select(col('tpep_dropoff_datetime').alias('timestamp')).\\\n",
    "    dropDuplicates()\n",
    "do_times.cache()\n",
    "print(\"datetime dim\")\n",
    "datetime_DIM = pu_times.\\\n",
    "    union(do_times).\\\n",
    "    dropDuplicates().\\\n",
    "    withColumn('datetime_key', monotonically_increasing_id())\n",
    "DIM_tables_dict['datetime_DIM'] = datetime_DIM\n",
    "\n",
    "## create locations DIM\n",
    "pu_locations = yellow_df.\\\n",
    "    select(col('pulocationid').alias('locationid')).\\\n",
    "    dropDuplicates()\n",
    "\n",
    "do_locations = yellow_df.\\\n",
    "    select(col('dolocationid').alias('locationid')).\\\n",
    "    dropDuplicates()\n",
    "\n",
    "taxizone_DIM = pu_locations.\\\n",
    "    union(do_locations).\\\n",
    "    dropDuplicates().\\\n",
    "    withColumn('zone_key', monotonically_increasing_id())\n",
    "DIM_tables_dict['taxizone_DIM'] = taxizone_DIM\n",
    "\n",
    "## create payment type DIM\n",
    "@udf\n",
    "def paymenttype_name_udf(payment_type):\n",
    "    payment_type_dict = {\n",
    "                    1 : 'Credit card',\n",
    "                    2 : 'Cash',\n",
    "                    3 : 'No charge',\n",
    "                    4 : 'Dispute',\n",
    "                    5 : 'Unknown',\n",
    "                    6 : 'Voided trip'\n",
    "    }\n",
    "    if payment_type is not None:\n",
    "        return payment_type_dict[payment_type]\n",
    "\n",
    "paymenttype_DIM = yellow_df.\\\n",
    "    select('payment_type').\\\n",
    "    distinct().\\\n",
    "    withColumn('paymenttype_key', monotonically_increasing_id()).\\\n",
    "    withColumn('description', paymenttype_name_udf(col('payment_type')))\n",
    "DIM_tables_dict['paymenttype_DIM'] = paymenttype_DIM\n",
    "\n",
    "## create rate code DIM\n",
    "@udf\n",
    "def ratecode_name_udf(ratecode):\n",
    "    ratecode_dict = {\n",
    "                    1 : 'Standard rate',\n",
    "                    2 : 'JFK',\n",
    "                    3 : 'Newark',\n",
    "                    4 : 'Nassau or Westchester',\n",
    "                    5 : 'Negotiated fare',\n",
    "                    6 : 'Group ride'\n",
    "    }\n",
    "    if ratecode is not None:\n",
    "        return ratecode_dict[ratecode]\n",
    "\n",
    "ratecode_DIM = yellow_df.\\\n",
    "    select('ratecodeid').\\\n",
    "    distinct().\\\n",
    "    withColumn('ratecode_key', monotonically_increasing_id()).\\\n",
    "    withColumn('description', ratecode_name_udf(col('ratecodeid')))\n",
    "DIM_tables_dict['ratecode_DIM'] = ratecode_DIM\n",
    "\n",
    "# import yaml\n",
    "# config = yaml.safe_load(open(\"./airflow/config/config.yaml\"))\n",
    "\n",
    "# connection_string = f\"jdbc:postgresql://{connection_params['host']}:{connection_params['port']}/{connection_params['database']}\"\n",
    "    \n",
    "for table_name in DIM_tables_dict.keys():\n",
    "    print(f\"saving table {table_name}\")\n",
    "\n",
    "    DIM_tables_dict[table_name].write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", connection_string) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", connection_params['user']) \\\n",
    "        .option(\"password\", connection_params['password']) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "## saving fact table\n",
    "fact_table.write\\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", connection_string) \\\n",
    "        .option(\"dbtable\", 'fact_table') \\\n",
    "        .option(\"user\", connection_params['user']) \\\n",
    "        .option(\"password\", connection_params['password']) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "\n",
    "# for table_name in DIM_tables_dict.keys():\n",
    "#         print(f\"writing table {table_name}\")\n",
    "        \n",
    "#         DIM_tables_dict[table_name].write \\\n",
    "#                 .format(\"parquet\") \\\n",
    "#                 .mode(\"overwrite\") \\\n",
    "#                 .save(f\"{table_name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d0a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
